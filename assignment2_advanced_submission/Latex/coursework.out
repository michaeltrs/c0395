\BOOKMARK [1][]{section.1}{1 Linear and Relu layers}{}% 1
\BOOKMARK [2][]{subsection.2}{1.1 Linear forward pass}{section.1}% 2
\BOOKMARK [2][]{subsection.4}{1.2 Linear backward pass}{section.1}% 3
\BOOKMARK [2][]{subsection.8}{1.3 Relu forward pass}{section.1}% 4
\BOOKMARK [2][]{subsection.10}{1.4 Relu backward pass}{section.1}% 5
\BOOKMARK [1][]{section.11}{2 Dropout}{}% 6
\BOOKMARK [2][]{subsection.12}{2.1 Dropout forward pass}{section.11}% 7
\BOOKMARK [2][]{subsection.13}{2.2 Dropout backward pass}{section.11}% 8
\BOOKMARK [1][]{section.14}{3 Softmax}{}% 9
\BOOKMARK [2][]{subsection.16}{3.1 Gradient of softmax}{section.14}% 10
\BOOKMARK [1][]{section.17}{4 Question 4}{}% 11
\BOOKMARK [1][]{section.20}{5 Hyper-parameter Optimization}{}% 12
\BOOKMARK [2][]{subsection.21}{5.1 Learning rate optimization}{section.20}% 13
\BOOKMARK [2][]{subsection.26}{5.2 Dropout optimization}{section.20}% 14
\BOOKMARK [2][]{subsection.28}{5.3 L2 regularization vs dropout}{section.20}% 15
\BOOKMARK [2][]{subsection.32}{5.4 Number and size of hidden layers}{section.20}% 16
\BOOKMARK [2][]{subsection.34}{5.5 Performance of NN on test data}{section.20}% 17
\BOOKMARK [1][]{section.37}{6 Question 6}{}% 18
\BOOKMARK [1][]{section.41}{7 Additional questions}{}% 19
\BOOKMARK [2][]{subsection.42}{7.1 A1}{section.41}% 20
\BOOKMARK [2][]{subsection.43}{7.2 A2}{section.41}% 21
\BOOKMARK [1][]{section.44}{8 Instructions for executing}{}% 22
\BOOKMARK [1][]{section.45}{9 References}{}% 23
